{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IjBruu7rDYOz"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models  # <-- Make sure to add this import for models\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Configuration and Settings\n",
        "# =========================\n",
        "\n",
        "class Config:\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Data paths\n",
        "    data_paths = {\n",
        "        '1_12': r'/content/FTD/Final Testing Dataset',\n",
        "        '1_4': r'/content/1-4/1-4',\n",
        "        '5_8': r'/content/5-8/5-8',\n",
        "        '9_12': r'/content/9-12/9-12',\n",
        "        'Top': r'/content/Top_level'\n",
        "    }\n",
        "\n",
        "    # Training parameters\n",
        "    training_params = {\n",
        "        'batch_size': 16,\n",
        "        'num_workers': 4,\n",
        "        'epochs': 50\n",
        "    }\n",
        "\n",
        "    # Learning rates for different models\n",
        "    learning_rates = {\n",
        "        '1_12': 0.001,\n",
        "        '1_4': 0.0001,\n",
        "        '5_8': 0.0001,\n",
        "        '9_12': 0.0001,\n",
        "        'Top': 0.0001\n",
        "    }\n",
        "\n",
        "    # Number of classes for each model\n",
        "    num_classes = {\n",
        "        '1_12': 12,\n",
        "        '1_4': 4,\n",
        "        '5_8': 4,\n",
        "        '9_12': 4,\n",
        "        'Top': 3\n",
        "    }\n",
        "\n",
        "    # Model save paths\n",
        "    model_save_paths = {\n",
        "        '1_12': '1_12_bats.pth',\n",
        "        '1_4': '1_4_model.pth',\n",
        "        '5_8': '5_8_model.pth',\n",
        "        '9_12': '9_12_model.pth',\n",
        "        'Top': 'top_model.pth'\n",
        "    }\n",
        "\n",
        "# Initialize configuration\n",
        "cfg = Config()\n",
        "\n"
      ],
      "metadata": {
        "id": "fDNmifR-4Y7G"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Data Handling Functions\n",
        "# =========================\n",
        "\n",
        "def get_data_transforms(train=True):\n",
        "    \"\"\"Define and return data transformations.\"\"\"\n",
        "    if train:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((224, 224)),  # Resize to standard size\n",
        "            transforms.RandomHorizontalFlip(),  # Random horizontal flipping\n",
        "            transforms.RandomRotation(45),  # Random rotation\n",
        "            transforms.ToTensor(),  # Convert to tensor\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((224, 224)),  # Resize to standard size\n",
        "            transforms.ToTensor(),  # Convert to tensor\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "        ])\n",
        "\n",
        "def load_dataset(root_dir, transform):\n",
        "    \"\"\"\n",
        "    Load dataset using ImageFolder.\n",
        "\n",
        "    Args:\n",
        "        root_dir (str): Path to the dataset directory.\n",
        "        transform (torchvision.transforms.Compose): Transformations to apply.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.data.Dataset: Loaded dataset.\n",
        "    \"\"\"\n",
        "    return datasets.ImageFolder(root=root_dir, transform=transform)\n",
        "\n",
        "def split_dataset(dataset, val_ratio=0.2, test_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Split dataset into training, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        dataset (torch.utils.data.Dataset): The dataset to split.\n",
        "        val_ratio (float): Fraction of data for validation.\n",
        "        test_ratio (float): Fraction of data for testing.\n",
        "\n",
        "    Returns:\n",
        "        tuple: train_dataset, val_dataset, test_dataset\n",
        "    \"\"\"\n",
        "    total_size = len(dataset)\n",
        "    val_size = int(val_ratio * total_size)\n",
        "    test_size = int(test_ratio * total_size)\n",
        "    train_size = total_size - val_size - test_size\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def get_dataloaders(train_dataset, val_dataset, test_dataset, batch_size, num_workers):\n",
        "    \"\"\"\n",
        "    Create DataLoaders for training, validation, and testing.\n",
        "\n",
        "    Args:\n",
        "        train_dataset (torch.utils.data.Dataset): Training dataset.\n",
        "        val_dataset (torch.utils.data.Dataset): Validation dataset.\n",
        "        test_dataset (torch.utils.data.Dataset): Test dataset.\n",
        "        batch_size (int): Batch size.\n",
        "        num_workers (int): Number of subprocesses for data loading.\n",
        "\n",
        "    Returns:\n",
        "        tuple: train_loader, val_loader, test_loader\n",
        "    \"\"\"\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "U4Pcol124h8A"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Training and Evaluation Functions\n",
        "# =========================\n",
        "\n",
        "def train_model(model, train_loader, val_loader, loss_fn, optimizer, device, epochs, model_name):\n",
        "    \"\"\"\n",
        "    Train the given model and validate after each epoch.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to train.\n",
        "        train_loader (DataLoader): DataLoader for training data.\n",
        "        val_loader (DataLoader): DataLoader for validation data.\n",
        "        loss_fn (nn.Module): Loss function.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer.\n",
        "        device (torch.device): Device to train on.\n",
        "        epochs (int): Number of epochs.\n",
        "        model_name (str): Name identifier for the model.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: Trained model.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        print(f\"Epoch: {epoch + 1}/{epochs} - Model: {model_name}\")\n",
        "        pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in pbar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({'Loss': f\"{running_loss / (total):.4f}\",\n",
        "                              'Accuracy': f\"{correct / total:.4f}\"})\n",
        "\n",
        "        # Validation after each epoch\n",
        "        val_acc, val_loss = evaluate_model(model, val_loader, loss_fn, device)\n",
        "        print(f\"Validation - Accuracy: {val_acc:.4f}, Loss: {val_loss:.4f}\\n\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, data_loader, loss_fn, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to evaluate.\n",
        "        data_loader (DataLoader): DataLoader for the dataset.\n",
        "        loss_fn (nn.Module): Loss function.\n",
        "        device (torch.device): Device to evaluate on.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Accuracy, Average Loss\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    return accuracy, avg_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "O5Vv-s_R4mHo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Combined Model Class\n",
        "# =========================\n",
        "\n",
        "class CombinedModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined model that uses a top-level model to decide which specialized model to use for each input.\n",
        "    \"\"\"\n",
        "    def __init__(self, top_model, specialized_models, device):\n",
        "        \"\"\"\n",
        "        Initialize the CombinedModel.\n",
        "\n",
        "        Args:\n",
        "            top_model (nn.Module): The top-level model.\n",
        "            specialized_models (dict): Dictionary of specialized models.\n",
        "            device (torch.device): Device to run the models on.\n",
        "        \"\"\"\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.top_model = top_model\n",
        "        self.specialized_models = nn.ModuleDict(specialized_models)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the combined model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Combined output tensor.\n",
        "        \"\"\"\n",
        "        # Get decisions from the top-level model\n",
        "        decision_logits = self.top_model(x)\n",
        "        decisions = torch.argmax(decision_logits, dim=1)  # Shape: (batch_size,)\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        total_classes = sum([cfg.num_classes[key] for key in self.specialized_models.keys()])\n",
        "        combined_outputs = torch.zeros(batch_size, total_classes).to(self.device)\n",
        "\n",
        "        # Process inputs in batches based on decisions\n",
        "        for decision, model_key in enumerate(self.specialized_models.keys()):\n",
        "            indices = (decisions == decision).nonzero(as_tuple=True)[0]\n",
        "            if len(indices) == 0:\n",
        "                continue  # No samples for this decision\n",
        "            subset = x[indices]\n",
        "            outputs = self.specialized_models[model_key](subset)\n",
        "            # Determine the class index offset\n",
        "            class_offset = sum([cfg.num_classes[key] for key in list(self.specialized_models.keys())[:decision]])\n",
        "            combined_outputs[indices, class_offset:class_offset + cfg.num_classes[model_key]] = outputs\n",
        "\n",
        "        return combined_outputs\n"
      ],
      "metadata": {
        "id": "EU3AeRuL4pax"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Train and Save Model Function\n",
        "# =========================\n",
        "\n",
        "\n",
        "def train_and_save_model(model_key, model, train_loader, val_loader):\n",
        "    \"\"\"\n",
        "    Train and save a model given its key, train_loader, and val_loader.\n",
        "\n",
        "    Args:\n",
        "        model_key (str): The identifier for the model (e.g., '1_4', '5_8', '9_12').\n",
        "        model (nn.Module): The model to train.\n",
        "        train_loader (DataLoader): DataLoader for training.\n",
        "        val_loader (DataLoader): DataLoader for validation.\n",
        "    \"\"\"\n",
        "    print(f\"Training model {model_key}...\")\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rates[model_key])\n",
        "\n",
        "    # Check if model file exists\n",
        "    model_path = cfg.model_save_paths[model_key]\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Loading existing {model_key} model...\")\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    else:\n",
        "        print(f\"Training new {model_key} model...\")\n",
        "        # Train the model\n",
        "        model = train_model(model, train_loader, val_loader, loss_fn, optimizer, cfg.device, epochs=cfg.training_params['epochs'], model_name=model_key)\n",
        "\n",
        "        # Save the model after training\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"{model_key} model saved successfully!\")"
      ],
      "metadata": {
        "id": "1nFSJUmwGRsw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Early Stopper Class\n",
        "# =========================\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How many epochs to wait after last time validation loss improved.\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.delta:\n",
        "            self.counter += 1\n",
        "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n"
      ],
      "metadata": {
        "id": "R-DppvXiF9QM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Main Execution Flow\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    # Define transformations for image processing\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize images\n",
        "        transforms.ToTensor(),  # Convert to tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "    ])\n",
        "\n",
        "    # Check and print the directory structure and file counts\n",
        "    print(\"Verifying dataset structure and files:\")\n",
        "    for root, dirs, files in os.walk(cfg.data_paths['1_12']):\n",
        "        print(root, len(files))  # Shows number of files in each directory\n",
        "\n",
        "    # Load the test dataset for combined evaluation\n",
        "    test_root_dir = cfg.data_paths['1_12']\n",
        "    test_dataset = load_dataset(test_root_dir, transform)\n",
        "    _, _, combined_test_loader = get_dataloaders(\n",
        "        *split_dataset(test_dataset),\n",
        "        cfg.training_params['batch_size'],\n",
        "        cfg.training_params['num_workers']\n",
        "    )\n",
        "\n",
        "    # Define the top-level model (you can choose any architecture or custom model)\n",
        "    top_model = models.resnet18(pretrained=False)  # Example: ResNet-18\n",
        "    top_model.fc = nn.Linear(top_model.fc.in_features, cfg.num_classes['Top'])  # Adjust the output for 3 classes\n",
        "\n",
        "    # Define specialized models (one for each subset of classes)\n",
        "    specialized_models = {\n",
        "        '1_4': models.resnet18(pretrained=False),\n",
        "        '5_8': models.resnet18(pretrained=False),\n",
        "        '9_12': models.resnet18(pretrained=False)\n",
        "    }\n",
        "\n",
        "    # Adjust the output layers of each specialized model\n",
        "    specialized_models['1_4'].fc = nn.Linear(specialized_models['1_4'].fc.in_features, cfg.num_classes['1_4'])\n",
        "    specialized_models['5_8'].fc = nn.Linear(specialized_models['5_8'].fc.in_features, cfg.num_classes['5_8'])\n",
        "    specialized_models['9_12'].fc = nn.Linear(specialized_models['9_12'].fc.in_features, cfg.num_classes['9_12'])\n",
        "\n",
        "    # Define the CombinedModel\n",
        "    combined_model = CombinedModel(top_model, specialized_models, cfg.device)\n",
        "    combined_model.to(cfg.device)\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()  # For classification tasks\n",
        "    optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\n",
        "\n",
        "    # Check if model file exists\n",
        "    model_path = \"combined_model.pth\"\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        print(\"Loading existing model...\")\n",
        "        combined_model.load_state_dict(torch.load(model_path))\n",
        "    else:\n",
        "        print(\"Training new model...\")\n",
        "        # Train the model if not already saved\n",
        "        combined_model = train_model(combined_model, combined_test_loader, combined_test_loader, loss_fn, optimizer, cfg.device, epochs=10, model_name='combined_model')\n",
        "\n",
        "        # Save the model after training\n",
        "        torch.save(combined_model.state_dict(), model_path)\n",
        "        print(\"Model saved successfully!\")\n",
        "\n",
        "    # Evaluate the combined model\n",
        "    evaluate_model(combined_model, combined_test_loader, loss_fn, cfg.device)\n",
        "\n",
        "    # Save the evaluated model\n",
        "    torch.save(combined_model.state_dict(), \"combined_model_evaluated.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "x0Jg_5bt4saS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef5d8de-50e9-4900-ef1c-4e59ab955a7c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying dataset structure and files:\n",
            "/content/FTD/Final Testing Dataset 0\n",
            "/content/FTD/Final Testing Dataset/12 500\n",
            "/content/FTD/Final Testing Dataset/9 500\n",
            "/content/FTD/Final Testing Dataset/1 501\n",
            "/content/FTD/Final Testing Dataset/8 500\n",
            "/content/FTD/Final Testing Dataset/5 500\n",
            "/content/FTD/Final Testing Dataset/4 500\n",
            "/content/FTD/Final Testing Dataset/3 500\n",
            "/content/FTD/Final Testing Dataset/2 500\n",
            "/content/FTD/Final Testing Dataset/10 500\n",
            "/content/FTD/Final Testing Dataset/11 500\n",
            "/content/FTD/Final Testing Dataset/6 500\n",
            "/content/FTD/Final Testing Dataset/7 500\n",
            "Training new model...\n",
            "Epoch: 1/10 - Model: combined_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 75/75 [00:06<00:00, 11.02it/s, Loss=0.1243, Accuracy=0.2250]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.0608, Loss: 4.6694\n",
            "\n",
            "Epoch: 2/10 - Model: combined_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 75/75 [00:06<00:00, 11.50it/s, Loss=0.1111, Accuracy=0.2667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.1500, Loss: 2.5988\n",
            "\n",
            "Epoch: 3/10 - Model: combined_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 75/75 [00:06<00:00, 11.93it/s, Loss=0.1086, Accuracy=0.2992]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.2650, Loss: 1.7616\n",
            "\n",
            "Epoch: 4/10 - Model: combined_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 75/75 [00:06<00:00, 11.55it/s, Loss=0.1065, Accuracy=0.3050]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.3233, Loss: 1.6867\n",
            "\n",
            "Epoch: 5/10 - Model: combined_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 75/75 [00:08<00:00,  9.35it/s, Loss=0.1008, Accuracy=0.3333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.3433, Loss: 1.6107\n",
            "\n",
            "Epoch: 6/10 - Model: combined_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 75/75 [00:07<00:00, 10.64it/s, Loss=0.1034, Accuracy=0.3292]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.2475, Loss: 1.9933\n",
            "\n",
            "Epoch: 7/10 - Model: combined_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 75/75 [00:06<00:00, 11.92it/s, Loss=0.1002, Accuracy=0.3350]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.3675, Loss: 1.5410\n",
            "\n",
            "Epoch: 8/10 - Model: combined_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 75/75 [00:06<00:00, 11.94it/s, Loss=0.1005, Accuracy=0.3333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.3092, Loss: 1.7846\n",
            "\n",
            "Epoch: 9/10 - Model: combined_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 75/75 [00:06<00:00, 10.77it/s, Loss=0.0977, Accuracy=0.3558]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.3733, Loss: 1.5271\n",
            "\n",
            "Epoch: 10/10 - Model: combined_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 75/75 [00:06<00:00, 11.36it/s, Loss=0.0966, Accuracy=0.3600]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.3558, Loss: 1.5512\n",
            "\n",
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Main Execution Flow (Updated with CombinedModel)\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    # Define transformations for image processing with augmentation for training\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize images\n",
        "        transforms.RandomHorizontalFlip(),  # Random horizontal flipping\n",
        "        transforms.RandomVerticalFlip(),  # Random vertical flipping\n",
        "        transforms.RandomRotation(45),  # Random rotation\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jittering\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop with resizing\n",
        "        transforms.ToTensor(),  # Convert to tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "    ])\n",
        "\n",
        "    # Transformation without augmentation for validation and testing\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize images\n",
        "        transforms.ToTensor(),  # Convert to tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "    ])\n",
        "\n",
        "    # Models and their corresponding datasets\n",
        "    model_info = {\n",
        "        '1_4': cfg.data_paths['1_4'],\n",
        "        '5_8': cfg.data_paths['5_8'],\n",
        "        '9_12': cfg.data_paths['9_12'],\n",
        "        '1_12': cfg.data_paths['1_12']\n",
        "    }\n",
        "\n",
        "    # Dictionary to hold trained specialized models\n",
        "    specialized_models = {}\n",
        "\n",
        "    # Train each specialized model separately\n",
        "    for model_key, dataset_path in model_info.items():\n",
        "        print(f\"\\nTraining model {model_key}...\")\n",
        "\n",
        "        # Load the dataset for this model with training and test/val transforms\n",
        "        dataset = load_dataset(dataset_path, train_transform)\n",
        "        train_loader, val_loader, test_loader = get_dataloaders(\n",
        "            *split_dataset(dataset),\n",
        "            cfg.training_params['batch_size'],\n",
        "            cfg.training_params['num_workers']\n",
        "        )\n",
        "\n",
        "        # Define the model with the appropriate number of classes\n",
        "        model = models.resnet18(pretrained=False)\n",
        "        model.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),  # Add dropout to reduce overfitting\n",
        "            nn.Linear(model.fc.in_features, cfg.num_classes[model_key])  # Adjust output layer\n",
        "        )\n",
        "\n",
        "        # Define loss function, optimizer, and scheduler\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rates[model_key])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "        early_stopper = EarlyStopping(patience=5, delta=0.01)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(cfg.training_params['epochs']):\n",
        "            print(f\"Epoch: {epoch + 1}/{cfg.training_params['epochs']} - Model: {model_key}\")\n",
        "\n",
        "            # Train for one epoch\n",
        "            model = train_model(model, train_loader, val_loader, loss_fn, optimizer, cfg.device, epochs=1, model_name=model_key)\n",
        "\n",
        "            # Validation after each epoch\n",
        "            val_acc, val_loss = evaluate_model(model, val_loader, loss_fn, cfg.device)\n",
        "            print(f\"Validation - Accuracy: {val_acc:.4f}, Loss: {val_loss:.4f}\")\n",
        "\n",
        "            # Adjust learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "            # Early stopping check\n",
        "            early_stopper(val_loss)\n",
        "            if early_stopper.early_stop:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        # Save the trained model\n",
        "        torch.save(model.state_dict(), cfg.model_save_paths[model_key])\n",
        "        print(f\"{model_key} model saved successfully!\")\n",
        "\n",
        "        # Store the trained model in the specialized_models dictionary\n",
        "        specialized_models[model_key] = model\n",
        "\n",
        "    # Define and train the top-level model\n",
        "    print(\"\\nTraining the top-level model (Top)...\")\n",
        "    top_dataset = load_dataset(cfg.data_paths['Top'], train_transform)\n",
        "    train_loader, val_loader, test_loader = get_dataloaders(\n",
        "        *split_dataset(top_dataset),\n",
        "        cfg.training_params['batch_size'],\n",
        "        cfg.training_params['num_workers']\n",
        "    )\n",
        "\n",
        "    # Define the top model\n",
        "    top_model = models.resnet18(pretrained=False)\n",
        "    top_model.fc = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(top_model.fc.in_features, cfg.num_classes['Top'])  # Adjust output for the top model\n",
        "    )\n",
        "\n",
        "    # Train the top model\n",
        "    top_model = train_model(top_model, train_loader, val_loader, loss_fn, optimizer, cfg.device, epochs=cfg.training_params['epochs'], model_name=\"Top\")\n",
        "    torch.save(top_model.state_dict(), cfg.model_save_paths['Top'])\n",
        "    print(\"Top-level model saved successfully!\")\n",
        "\n",
        "    # Now, combine the top-level model and specialized models\n",
        "    print(\"\\nCombining the top-level model with specialized models...\")\n",
        "    combined_model = CombinedModel(top_model, specialized_models, cfg.device)\n",
        "    combined_model.to(cfg.device)\n",
        "\n",
        "    # Evaluate the combined model on the test set of the top-level data\n",
        "    print(f\"\\nEvaluating the combined model on test data...\")\n",
        "    test_acc, test_loss = evaluate_model(combined_model, test_loader, loss_fn, cfg.device)\n",
        "    print(f\"Combined Model - Test Accuracy: {test_acc:.4f}, Loss: {test_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivtAREUNOgd9",
        "outputId": "2c249036-3d90-4bc9-ee33-827e4b8a0c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model 1_4...\n",
            "Epoch: 1/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  8.07it/s, Loss=0.0631, Accuracy=0.4879]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6850, Loss: 0.6964\n",
            "\n",
            "Validation - Accuracy: 0.6500, Loss: 0.6844\n",
            "Epoch: 2/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  8.34it/s, Loss=0.0473, Accuracy=0.6578]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6975, Loss: 0.6297\n",
            "\n",
            "Validation - Accuracy: 0.6950, Loss: 0.6524\n",
            "Epoch: 3/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  8.04it/s, Loss=0.0443, Accuracy=0.6844]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6925, Loss: 0.7685\n",
            "\n",
            "Validation - Accuracy: 0.6900, Loss: 0.7319\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch: 4/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  8.07it/s, Loss=0.0382, Accuracy=0.7544]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6550, Loss: 0.7971\n",
            "\n",
            "Validation - Accuracy: 0.6275, Loss: 0.8786\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch: 5/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.65it/s, Loss=0.0359, Accuracy=0.7627]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.7400, Loss: 0.5705\n",
            "\n",
            "Validation - Accuracy: 0.7700, Loss: 0.5353\n",
            "Epoch: 6/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.93it/s, Loss=0.0325, Accuracy=0.7877]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.7250, Loss: 0.6901\n",
            "\n",
            "Validation - Accuracy: 0.7100, Loss: 0.7201\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch: 7/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.80it/s, Loss=0.0334, Accuracy=0.7910]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.8275, Loss: 0.3955\n",
            "\n",
            "Validation - Accuracy: 0.8200, Loss: 0.3993\n",
            "Epoch: 8/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.95it/s, Loss=0.0302, Accuracy=0.8018]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.8225, Loss: 0.4206\n",
            "\n",
            "Validation - Accuracy: 0.7950, Loss: 0.4276\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch: 9/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.86it/s, Loss=0.0296, Accuracy=0.8251]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.7650, Loss: 0.7159\n",
            "\n",
            "Validation - Accuracy: 0.7625, Loss: 0.6287\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch: 10/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.88it/s, Loss=0.0292, Accuracy=0.8110]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.8475, Loss: 0.3934\n",
            "\n",
            "Validation - Accuracy: 0.8525, Loss: 0.3633\n",
            "Epoch: 11/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.83it/s, Loss=0.0261, Accuracy=0.8426]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.8925, Loss: 0.2492\n",
            "\n",
            "Validation - Accuracy: 0.9050, Loss: 0.2527\n",
            "Epoch: 12/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.96it/s, Loss=0.0239, Accuracy=0.8535]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9200, Loss: 0.2323\n",
            "\n",
            "Validation - Accuracy: 0.9000, Loss: 0.2653\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch: 13/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.87it/s, Loss=0.0224, Accuracy=0.8651]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9275, Loss: 0.2204\n",
            "\n",
            "Validation - Accuracy: 0.9150, Loss: 0.2272\n",
            "Epoch: 14/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.88it/s, Loss=0.0240, Accuracy=0.8560]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9050, Loss: 0.2448\n",
            "\n",
            "Validation - Accuracy: 0.9050, Loss: 0.2379\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch: 15/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.87it/s, Loss=0.0233, Accuracy=0.8793]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9225, Loss: 0.2264\n",
            "\n",
            "Validation - Accuracy: 0.9125, Loss: 0.2114\n",
            "Epoch: 16/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.90it/s, Loss=0.0229, Accuracy=0.8618]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9100, Loss: 0.2386\n",
            "\n",
            "Validation - Accuracy: 0.8925, Loss: 0.2663\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch: 17/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.90it/s, Loss=0.0219, Accuracy=0.8684]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9225, Loss: 0.2025\n",
            "\n",
            "Validation - Accuracy: 0.9225, Loss: 0.2036\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch: 18/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.91it/s, Loss=0.0233, Accuracy=0.8568]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9325, Loss: 0.2000\n",
            "\n",
            "Validation - Accuracy: 0.9300, Loss: 0.2029\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch: 19/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.97it/s, Loss=0.0205, Accuracy=0.8668]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9275, Loss: 0.2099\n",
            "\n",
            "Validation - Accuracy: 0.9350, Loss: 0.2010\n",
            "Epoch: 20/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.86it/s, Loss=0.0225, Accuracy=0.8576]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9275, Loss: 0.1909\n",
            "\n",
            "Validation - Accuracy: 0.9375, Loss: 0.1884\n",
            "Epoch: 21/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.97it/s, Loss=0.0201, Accuracy=0.8868]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9275, Loss: 0.2051\n",
            "\n",
            "Validation - Accuracy: 0.9225, Loss: 0.1940\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch: 22/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.90it/s, Loss=0.0206, Accuracy=0.8776]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9325, Loss: 0.1948\n",
            "\n",
            "Validation - Accuracy: 0.9375, Loss: 0.1796\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch: 23/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.97it/s, Loss=0.0216, Accuracy=0.8709]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9100, Loss: 0.2257\n",
            "\n",
            "Validation - Accuracy: 0.8900, Loss: 0.2490\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch: 24/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.97it/s, Loss=0.0197, Accuracy=0.8768]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9325, Loss: 0.2068\n",
            "\n",
            "Validation - Accuracy: 0.9250, Loss: 0.1971\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch: 25/50 - Model: 1_4\n",
            "Epoch: 1/1 - Model: 1_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 76/76 [00:09<00:00,  7.99it/s, Loss=0.0218, Accuracy=0.8734]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9325, Loss: 0.1850\n",
            "\n",
            "Validation - Accuracy: 0.9325, Loss: 0.1923\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping triggered.\n",
            "1_4 model saved successfully!\n",
            "\n",
            "Training model 5_8...\n",
            "Epoch: 1/50 - Model: 5_8\n",
            "Epoch: 1/1 - Model: 5_8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1:  79%|███████▊  | 59/75 [00:07<00:01, 10.25it/s, Loss=0.0831, Accuracy=0.3930]"
          ]
        }
      ]
    }
  ]
}