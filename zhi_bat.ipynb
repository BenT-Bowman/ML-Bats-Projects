{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "IjBruu7rDYOz",
        "outputId": "a9e0446f-67ba-4640-cdba-6028f3400e72"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fDNmifR-4Y7G"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Configuration and Settings\n",
        "# =========================\n",
        "\n",
        "class Config:\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Data paths\n",
        "    data_paths = {\n",
        "        '1_12': r'Bat/Final Testing Dataset',\n",
        "        '1_4': r'Bat/1-4',\n",
        "        '5_8': r'Bat/5-8',\n",
        "        '9_12': r'Bat/9-12',\n",
        "        'Top': r'Data\\Top_level'\n",
        "    }\n",
        "\n",
        "    # Training parameters\n",
        "    training_params = {\n",
        "        'batch_size': 32,\n",
        "        'num_workers': 4,\n",
        "        'epochs': 50\n",
        "    }\n",
        "\n",
        "    # Learning rates for different models\n",
        "    learning_rates = {\n",
        "        '1_12': 0.001,\n",
        "        '1_4': 0.0001,\n",
        "        '5_8': 0.0001,\n",
        "        '9_12': 0.0001,\n",
        "        'Top': 0.0001\n",
        "    }\n",
        "\n",
        "    # Number of classes for each model\n",
        "    num_classes = {\n",
        "        '1_12': 12,\n",
        "        '1_4': 4,\n",
        "        '5_8': 4,\n",
        "        '9_12': 4,\n",
        "        'Top': 3\n",
        "    }\n",
        "\n",
        "    # Model save paths\n",
        "    model_save_paths = {\n",
        "        '1_12': '1_12_bats.pth',\n",
        "        '1_4': '1_4_model.pth',\n",
        "        '5_8': '5_8_model.pth',\n",
        "        '9_12': '9_12_model.pth',\n",
        "        'Top': 'top_model.pth'\n",
        "    }\n",
        "\n",
        "# Initialize configuration\n",
        "cfg = Config()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Pcol124h8A"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Data Handling Functions\n",
        "# =========================\n",
        "\n",
        "def get_data_transforms():\n",
        "    \"\"\"Define and return data transformations.\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomRotation(45)\n",
        "    ])\n",
        "\n",
        "def load_dataset(root_dir, transform):\n",
        "    \"\"\"\n",
        "    Load dataset using ImageFolder.\n",
        "\n",
        "    Args:\n",
        "        root_dir (str): Path to the dataset directory.\n",
        "        transform (torchvision.transforms.Compose): Transformations to apply.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.data.Dataset: Loaded dataset.\n",
        "    \"\"\"\n",
        "    return datasets.ImageFolder(root=root_dir, transform=transform)\n",
        "\n",
        "def split_dataset(dataset, val_ratio=0.2, test_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Split dataset into training, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        dataset (torch.utils.data.Dataset): The dataset to split.\n",
        "        val_ratio (float): Fraction of data for validation.\n",
        "        test_ratio (float): Fraction of data for testing.\n",
        "\n",
        "    Returns:\n",
        "        tuple: train_dataset, val_dataset, test_dataset\n",
        "    \"\"\"\n",
        "    total_size = len(dataset)\n",
        "    val_size = int(val_ratio * total_size)\n",
        "    test_size = int(test_ratio * total_size)\n",
        "    train_size = total_size - val_size - test_size\n",
        "    return random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "def get_dataloaders(train_dataset, val_dataset, test_dataset, batch_size, num_workers):\n",
        "    \"\"\"\n",
        "    Create DataLoaders for training, validation, and testing.\n",
        "\n",
        "    Args:\n",
        "        train_dataset (torch.utils.data.Dataset): Training dataset.\n",
        "        val_dataset (torch.utils.data.Dataset): Validation dataset.\n",
        "        test_dataset (torch.utils.data.Dataset): Test dataset.\n",
        "        batch_size (int): Batch size.\n",
        "        num_workers (int): Number of subprocesses for data loading.\n",
        "\n",
        "    Returns:\n",
        "        tuple: train_loader, val_loader, test_loader\n",
        "    \"\"\"\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5Vv-s_R4mHo"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Training and Evaluation Functions\n",
        "# =========================\n",
        "\n",
        "def train_model(model, train_loader, val_loader, loss_fn, optimizer, device, epochs, model_name):\n",
        "    \"\"\"\n",
        "    Train the given model and validate after each epoch.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to train.\n",
        "        train_loader (DataLoader): DataLoader for training data.\n",
        "        val_loader (DataLoader): DataLoader for validation data.\n",
        "        loss_fn (nn.Module): Loss function.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer.\n",
        "        device (torch.device): Device to train on.\n",
        "        epochs (int): Number of epochs.\n",
        "        model_name (str): Name identifier for the model.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: Trained model.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        print(f\"Epoch: {epoch + 1}/{epochs} - Model: {model_name}\")\n",
        "        pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in pbar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({'Loss': f\"{running_loss / (total):.4f}\",\n",
        "                              'Accuracy': f\"{correct / total:.4f}\"})\n",
        "\n",
        "        # Validation after each epoch\n",
        "        val_acc, val_loss = evaluate_model(model, val_loader, loss_fn, device)\n",
        "        print(f\"Validation - Accuracy: {val_acc:.4f}, Loss: {val_loss:.4f}\\n\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, data_loader, loss_fn, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to evaluate.\n",
        "        data_loader (DataLoader): DataLoader for the dataset.\n",
        "        loss_fn (nn.Module): Loss function.\n",
        "        device (torch.device): Device to evaluate on.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Accuracy, Average Loss\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    return accuracy, avg_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU3AeRuL4pax"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Combined Model Class\n",
        "# =========================\n",
        "\n",
        "class CombinedModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined model that uses a top-level model to decide which specialized model to use for each input.\n",
        "    \"\"\"\n",
        "    def __init__(self, top_model, specialized_models, device):\n",
        "        \"\"\"\n",
        "        Initialize the CombinedModel.\n",
        "\n",
        "        Args:\n",
        "            top_model (nn.Module): The top-level model.\n",
        "            specialized_models (dict): Dictionary of specialized models.\n",
        "            device (torch.device): Device to run the models on.\n",
        "        \"\"\"\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.top_model = top_model\n",
        "        self.specialized_models = nn.ModuleDict(specialized_models)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the combined model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Combined output tensor.\n",
        "        \"\"\"\n",
        "        # Get decisions from the top-level model\n",
        "        decision_logits = self.top_model(x)\n",
        "        decisions = torch.argmax(decision_logits, dim=1)  # Shape: (batch_size,)\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        total_classes = sum([cfg.num_classes[key] for key in self.specialized_models.keys()])\n",
        "        combined_outputs = torch.zeros(batch_size, total_classes).to(self.device)\n",
        "\n",
        "        # Process inputs in batches based on decisions\n",
        "        for decision, model_key in enumerate(self.specialized_models.keys()):\n",
        "            indices = (decisions == decision).nonzero(as_tuple=True)[0]\n",
        "            if len(indices) == 0:\n",
        "                continue  # No samples for this decision\n",
        "            subset = x[indices]\n",
        "            outputs = self.specialized_models[model_key](subset)\n",
        "            # Determine the class index offset\n",
        "            class_offset = sum([cfg.num_classes[key] for key in list(self.specialized_models.keys())[:decision]])\n",
        "            combined_outputs[indices, class_offset:class_offset + cfg.num_classes[model_key]] = outputs\n",
        "\n",
        "        return combined_outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0Jg_5bt4saS"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Main Execution Flow\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    # Define data transformations\n",
        "    transform = get_data_transforms()\n",
        "\n",
        "    # Dictionary to hold trained specialized models\n",
        "    trained_models = {}\n",
        "\n",
        "    # Train Specialized Models\n",
        "    for model_key in ['1_12', '1_4', '5_8', '9_12', 'Top']:\n",
        "        root_dir = cfg.data_paths[model_key]\n",
        "        dataset = load_dataset(root_dir, transform)\n",
        "        train_ds, val_ds, test_ds = split_dataset(dataset)\n",
        "        train_loader, val_loader, test_loader = get_dataloaders(\n",
        "            train_ds, val_ds, test_ds,\n",
        "            cfg.training_params['batch_size'],\n",
        "            cfg.training_params['num_workers']\n",
        "        )\n",
        "\n",
        "        # Initialize model\n",
        "        model = CNN(cfg.num_classes[model_key])\n",
        "\n",
        "        # Define loss and optimizer\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rates[model_key])\n",
        "\n",
        "        # Train the model\n",
        "        trained_model = train_model(\n",
        "            model, train_loader, val_loader,\n",
        "            loss_fn, optimizer,\n",
        "            cfg.device,\n",
        "            cfg.training_params['epochs'],\n",
        "            model_key\n",
        "        )\n",
        "\n",
        "        # Save the trained model's state_dict\n",
        "        torch.save(trained_model.state_dict(), cfg.model_save_paths[model_key])\n",
        "        print(f\"Model {model_key} saved to {cfg.model_save_paths[model_key]}\\n\")\n",
        "\n",
        "        # Store the trained model\n",
        "        trained_models[model_key] = trained_model\n",
        "\n",
        "    # Initialize Combined Model\n",
        "    specialized_model_keys = ['1_4', '5_8', '9_12']\n",
        "    specialized_models = {key: trained_models[key] for key in specialized_model_keys}\n",
        "    combined_model = CombinedModel(\n",
        "        top_model=trained_models['Top'],\n",
        "        specialized_models=specialized_models,\n",
        "        device=cfg.device\n",
        "    ).to(cfg.device)\n",
        "\n",
        "    # Load state_dicts for specialized models if not already loaded\n",
        "    for key in specialized_model_keys:\n",
        "        combined_model.specialized_models[key].to(cfg.device)\n",
        "\n",
        "    # Define loss function for combined model\n",
        "    combined_loss_fn = nn.CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a35fNGf44udy"
      },
      "outputs": [],
      "source": [
        "    # =========================\n",
        "    # Evaluation of Combined Model\n",
        "    # =========================\n",
        "\n",
        "    # Load the test dataset for combined evaluation\n",
        "    test_root_dir = cfg.data_paths['1_12']\n",
        "    test_dataset = load_dataset(test_root_dir, transform)\n",
        "    _, _, combined_test_ds = split_dataset(test_dataset)\n",
        "    _, _, combined_test_loader = get_dataloaders(\n",
        "        *split_dataset(test_dataset),\n",
        "        cfg.training_params['batch_size'],\n",
        "        cfg.training_params['num_workers']\n",
        "    )\n",
        "\n",
        "    # Evaluate the combined model\n",
        "    combined_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(combined_test_loader, desc=\"Evaluating Combined Model\"):\n",
        "            inputs, labels = inputs.to(cfg.device), labels.to(cfg.device)\n",
        "            outputs = combined_model(inputs)\n",
        "            loss = combined_loss_fn(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    test_accuracy = correct / total\n",
        "    test_avg_loss = total_loss / len(combined_test_loader)\n",
        "    print(f\"Combined Model - Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_avg_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
